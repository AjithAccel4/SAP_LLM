name: Integration Tests (Real Models - GPU)

on:
  # Run nightly
  schedule:
    - cron: '0 2 * * *'  # 2 AM UTC daily

  # Manual trigger
  workflow_dispatch:
    inputs:
      test_subset:
        description: 'Test subset to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - loading
          - inference
          - accuracy
          - performance
          - error_handling

  # Run on specific branches
  push:
    branches:
      - main
      - 'release/**'
    paths:
      - 'sap_llm/models/**'
      - 'tests/integration/test_real_*.py'
      - '.github/workflows/integration-tests-gpu.yml'

  # Run on PRs to main
  pull_request:
    branches:
      - main
    paths:
      - 'sap_llm/models/**'
      - 'tests/integration/test_real_*.py'

# Ensure only one GPU test runs at a time
concurrency:
  group: gpu-integration-tests
  cancel-in-progress: false  # Don't cancel - GPU tests are expensive

jobs:
  # ============================================================================
  # Pre-flight checks
  # ============================================================================
  preflight:
    name: Pre-flight Checks
    runs-on: ubuntu-latest
    outputs:
      should_run: ${{ steps.check.outputs.should_run }}
    steps:
      - uses: actions/checkout@v4

      - name: Check if GPU tests should run
        id: check
        run: |
          # Skip on forks (they don't have GPU runners)
          if [ "${{ github.event.pull_request.head.repo.fork }}" == "true" ]; then
            echo "should_run=false" >> $GITHUB_OUTPUT
            echo "Skipping GPU tests on fork"
            exit 0
          fi

          # Run on schedule, manual, or main branch
          if [ "${{ github.event_name }}" == "schedule" ] || \
             [ "${{ github.event_name }}" == "workflow_dispatch" ] || \
             [ "${{ github.ref }}" == "refs/heads/main" ]; then
            echo "should_run=true" >> $GITHUB_OUTPUT
          else
            echo "should_run=false" >> $GITHUB_OUTPUT
          fi

  # ============================================================================
  # Model Download (runs once, caches results)
  # ============================================================================
  download-models:
    name: Download Models
    needs: preflight
    if: needs.preflight.outputs.should_run == 'true'
    runs-on: [self-hosted, gpu, cuda]  # GPU runner with CUDA
    timeout-minutes: 120  # 2 hours for downloading large models

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install huggingface_hub transformers[torch] accelerate bitsandbytes

      - name: Check GPU
        run: |
          nvidia-smi
          python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
          python -c "import torch; print(f'CUDA devices: {torch.cuda.device_count()}')"
          python -c "import torch; print(f'CUDA version: {torch.version.cuda}')"

      - name: Cache models
        id: cache-models
        uses: actions/cache@v4
        with:
          path: /models/huggingface_cache
          key: sap-llm-models-v1-${{ hashFiles('config/models.yaml') }}
          restore-keys: |
            sap-llm-models-v1-

      - name: Download models
        if: steps.cache-models.outputs.cache-hit != 'true'
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: |
          python scripts/download_models.py \
            --cache-dir /models/huggingface_cache \
            --models vision_encoder language_decoder reasoning_engine \
            --max-retries 5

      - name: Verify models downloaded
        run: |
          python -c "
          from tests.utils.model_loader import check_models_downloaded
          status = check_models_downloaded()
          print('Model download status:', status)
          assert all(status.values()), f'Missing models: {[k for k,v in status.items() if not v]}'
          "

  # ============================================================================
  # Real Model Integration Tests
  # ============================================================================
  integration-tests:
    name: Integration Tests - ${{ matrix.test_suite }}
    needs: download-models
    runs-on: [self-hosted, gpu, cuda]
    timeout-minutes: 60

    strategy:
      fail-fast: false
      matrix:
        test_suite:
          - loading
          - inference
          - accuracy
          - performance
          - error_handling

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-timeout pytest-asyncio pytest-cov
          pip install transformers[torch] accelerate bitsandbytes pillow

      - name: Restore model cache
        uses: actions/cache@v4
        with:
          path: /models/huggingface_cache
          key: sap-llm-models-v1-${{ hashFiles('config/models.yaml') }}

      - name: Generate test fixtures
        run: |
          python tests/fixtures/create_test_documents.py

      - name: Run integration tests - ${{ matrix.test_suite }}
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_CACHE_DIR: /models/huggingface_cache
          CUDA_VISIBLE_DEVICES: 0
        run: |
          case "${{ matrix.test_suite }}" in
            loading)
              pytest tests/integration/test_real_models.py::TestRealModelLoading \
                -v -s --tb=short --timeout=600 \
                --junitxml=test-results-loading.xml
              ;;
            inference)
              pytest tests/integration/test_real_models.py::TestRealModelInference \
                -v -s --tb=short --timeout=600 \
                --junitxml=test-results-inference.xml
              ;;
            accuracy)
              pytest tests/integration/test_real_model_accuracy.py \
                -v -s --tb=short --timeout=1200 \
                --junitxml=test-results-accuracy.xml
              ;;
            performance)
              pytest tests/integration/test_real_model_performance.py \
                -v -s --tb=short --timeout=1200 \
                --junitxml=test-results-performance.xml
              ;;
            error_handling)
              pytest tests/integration/test_real_model_error_handling.py \
                -v -s --tb=short --timeout=600 \
                --junitxml=test-results-error-handling.xml
              ;;
          esac

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ matrix.test_suite }}
          path: test-results-*.xml

      - name: GPU Memory Report
        if: always()
        run: |
          nvidia-smi
          python -c "
          import torch
          if torch.cuda.is_available():
              print(f'GPU Memory Allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB')
              print(f'GPU Memory Reserved: {torch.cuda.memory_reserved()/1e9:.2f} GB')
              print(f'GPU Memory Max Allocated: {torch.cuda.max_memory_allocated()/1e9:.2f} GB')
          "

  # ============================================================================
  # End-to-End Tests
  # ============================================================================
  e2e-tests:
    name: End-to-End Pipeline Tests
    needs: download-models
    runs-on: [self-hosted, gpu, cuda]
    timeout-minutes: 90

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-timeout pytest-asyncio
          pip install transformers[torch] accelerate bitsandbytes pillow

      - name: Restore model cache
        uses: actions/cache@v4
        with:
          path: /models/huggingface_cache
          key: sap-llm-models-v1-${{ hashFiles('config/models.yaml') }}

      - name: Generate test fixtures
        run: |
          python tests/fixtures/create_test_documents.py

      - name: Run E2E tests
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          HF_CACHE_DIR: /models/huggingface_cache
          CUDA_VISIBLE_DEVICES: 0
        run: |
          pytest tests/integration/test_real_models.py::TestRealPipelineE2E \
            -v -s --tb=short --timeout=1800 \
            --junitxml=test-results-e2e.xml

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-e2e
          path: test-results-e2e.xml

  # ============================================================================
  # Test Report Summary
  # ============================================================================
  test-report:
    name: Generate Test Report
    needs: [integration-tests, e2e-tests]
    if: always()
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Download all test results
        uses: actions/download-artifact@v4
        with:
          path: test-results

      - name: Publish test results
        uses: EnricoMi/publish-unit-test-result-action@v2
        if: always()
        with:
          files: test-results/**/*.xml
          check_name: Real Model Integration Test Results
          comment_title: Real Model Integration Test Results

      - name: Create test summary
        if: always()
        run: |
          echo "## Real Model Integration Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Test Suites" >> $GITHUB_STEP_SUMMARY
          echo "- ✅ Model Loading Tests" >> $GITHUB_STEP_SUMMARY
          echo "- ✅ Inference Tests" >> $GITHUB_STEP_SUMMARY
          echo "- ✅ Accuracy Tests" >> $GITHUB_STEP_SUMMARY
          echo "- ✅ Performance Tests" >> $GITHUB_STEP_SUMMARY
          echo "- ✅ Error Handling Tests" >> $GITHUB_STEP_SUMMARY
          echo "- ✅ End-to-End Tests" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Performance Metrics" >> $GITHUB_STEP_SUMMARY
          echo "See individual test artifacts for detailed metrics" >> $GITHUB_STEP_SUMMARY

  # ============================================================================
  # Cleanup
  # ============================================================================
  cleanup:
    name: Cleanup GPU Resources
    needs: [integration-tests, e2e-tests]
    if: always()
    runs-on: [self-hosted, gpu, cuda]

    steps:
      - name: Clear GPU cache
        run: |
          python -c "
          import torch
          if torch.cuda.is_available():
              torch.cuda.empty_cache()
              print('GPU cache cleared')
          "

      - name: GPU Status
        run: |
          nvidia-smi
