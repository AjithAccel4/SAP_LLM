# SAP_LLM Model Configuration
# Configuration for real ML models used in integration tests and production

# Vision Encoder (LayoutLMv3) - Document Understanding
vision_encoder:
  model: microsoft/layoutlmv3-base
  path: models/layoutlmv3
  device: cuda:0
  precision: float16  # fp16 for faster inference
  batch_size: 8
  cache_dir: /models/huggingface_cache
  quantization: null  # Options: null, 8bit, 4bit
  max_sequence_length: 512
  num_labels: 15  # Number of document types

  # Training configuration
  training:
    learning_rate: 5e-5
    batch_size: 16
    epochs: 10
    warmup_steps: 500
    gradient_accumulation_steps: 2

# Language Decoder (LLaMA-2) - Field Extraction
language_decoder:
  model: meta-llama/Llama-2-7b-hf
  path: models/llama2
  device: cuda:0
  precision: float16
  batch_size: 4
  cache_dir: /models/huggingface_cache
  quantization: 8bit  # Use 8-bit quantization to save memory
  max_length: 2048
  temperature: 0.1  # Low temperature for deterministic extraction
  top_p: 0.9
  top_k: 50

  # LoRA configuration for fine-tuning
  lora:
    enabled: true
    r: 16
    lora_alpha: 32
    lora_dropout: 0.1
    target_modules:
      - q_proj
      - v_proj
      - k_proj
      - o_proj

  # Training configuration
  training:
    learning_rate: 2e-4
    batch_size: 8
    epochs: 3
    warmup_steps: 100
    gradient_accumulation_steps: 4

# Reasoning Engine (Mixtral) - Routing Decisions
reasoning_engine:
  model: mistralai/Mixtral-8x7B-v0.1
  path: models/mixtral
  device: cuda:1  # Use second GPU if available
  precision: float16
  batch_size: 2
  cache_dir: /models/huggingface_cache
  quantization: 4bit  # Use 4-bit quantization for large model
  max_length: 4096
  temperature: 0.2
  top_p: 0.95

  # Expert configuration
  num_experts: 8
  num_experts_per_tok: 2

  # Training configuration
  training:
    learning_rate: 1e-4
    batch_size: 4
    epochs: 2
    warmup_steps: 50
    gradient_accumulation_steps: 8

# OCR Model (Optional - for preprocessing)
ocr:
  model: microsoft/trocr-base-handwritten
  path: models/trocr
  device: cuda:0
  precision: float16
  batch_size: 16
  cache_dir: /models/huggingface_cache

# Performance Settings
performance:
  use_flash_attention: true  # Enable Flash Attention 2 for faster inference
  use_torch_compile: false    # Enable torch.compile (PyTorch 2.0+)
  use_bettertransformer: true # Enable BetterTransformer optimizations
  gradient_checkpointing: true # Save memory during training

# Memory Management
memory:
  max_memory_per_gpu_gb: 40  # Maximum memory per GPU
  cpu_offload: false          # Offload to CPU if GPU memory insufficient
  disk_offload: false         # Offload to disk (very slow)

# Inference Settings
inference:
  max_batch_size: 8
  timeout_seconds: 30
  retry_attempts: 3
  retry_delay_seconds: 2

# Test Configuration
test:
  use_quantization: true  # Use quantized models for faster tests
  max_test_batch_size: 4
  gpu_memory_fraction: 0.8
  enable_profiling: false

# Download Settings
download:
  token_env_var: HF_TOKEN
  resume_download: true
  max_retries: 3
  retry_delay_seconds: 5
  verify_checksums: true
